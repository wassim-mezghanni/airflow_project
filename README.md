# ğŸš– Weather Impact on Taxi Demand â€“ Airflow Data Pipeline  

## ğŸ“Œ Project Overview  
This project demonstrates how to build an **automated, production-style data pipeline** using **Apache Airflow**.  
The pipeline ingests **NYC Taxi trip data (CSV)** and **daily weather data (OpenWeather API)**, joins them, and stores the results in **PostgreSQL** for downstream analysis.  

Portfolio Highlight:  
> â€œAutomated daily pipeline combining API + historical data using Airflow for orchestration.â€  

---

## âš™ï¸ Tech Stack  
- **Python** ğŸ â€“ data processing & API calls  
- **Apache Airflow** â€“ DAG scheduling & orchestration  
- **PostgreSQL** â€“ data warehouse / storage  
- **Docker & docker-compose** â€“ containerized environment  
- **OpenWeather API** â€“ external weather data  

---

## ğŸ“‚ Project Structure  
```
.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                   # you create this with secrets/config
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ taxi_weather_dag.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ nyc-taxi-trip-duration.csv  # sample file (see notes on daily filenames)
â””â”€â”€ scripts/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ extract_taxi_data.py
    â”œâ”€â”€ fetch_weather_data.py
    â”œâ”€â”€ join_and_transform.py
    â””â”€â”€ load_to_postgres.py
```

---

## Getting Started

1) Prerequisites
- Docker Desktop and docker-compose
- OpenWeather API key: https://openweathermap.org/api

2) Create .env
```
WEATHER_API_KEY=YOUR_OPENWEATHER_KEY
WEATHER_CITY=New York,US
```

3) Wire env vars and data volume to Airflow containers
- In docker-compose.yml, for both webserver and scheduler services:
  - Add environment entries so your scripts can read the API settings:
    - WEATHER_API_KEY=${WEATHER_API_KEY}
    - WEATHER_CITY=${WEATHER_CITY}
  - Mount the data folder so the DAG can read taxi CSVs:
    - volumes: - ./data:/data

4) Start the stack
```
docker compose up -d
```
- UI: http://localhost:8080 (user: admin, pass: admin)

5) Run the pipeline
- In the Airflow UI, unpause the DAG: taxi_weather_pipeline
- Trigger a run or wait for the daily schedule

---

## How It Works
- Task 1: extract_taxi_data â†’ reads daily taxi CSV and writes /tmp/taxi_data.csv
- Task 2: fetch_weather_data â†’ fetches weather via OpenWeather and writes /tmp/weather_data.csv
- Task 3: join_and_transform â†’ merges by date and writes /tmp/final_data.csv
- Task 4: load_to_postgres â†’ appends final data to Postgres table taxi_weather

Pipeline graph:

![Airflow DAG â€“ taxi_weather_pipeline](assets/taxi-weather-pipeline.png)

Notes
- Weather API: the script fetches current weather and stamps it to â€œyesterdayâ€ to align with taxi data.
- File paths are inside the container; /tmp is ephemeral per container run.

---

## Configuration
- WEATHER_API_KEY and WEATHER_CITY are read by scripts/fetch_weather_data.py from environment variables.
- Postgres connection (inside containers): postgresql://airflow:airflow@postgres:5432/airflow
- Table name: taxi_weather (if_exists="append")

---

## Input Data Expectations
- extract_taxi_data expects a dated CSV under /data with pattern:
  - /data/nyc-taxi-trip-durationYYYY-MM-DD.csv (example: nyc-taxi-trip-duration2025-08-22.csv)
- For quick testing you can copy the sample to a dated filename:
  - cp data/nyc-taxi-trip-duration.csv data/nyc-taxi-trip-duration$(date -v-1d +%Y-%m-%d).csv  # macOS

Ensure the data directory is mounted into the containers at /data as noted above.

---

## Troubleshooting
- WEATHER_API_KEY is not set â†’ fetch_weather_data will raise a RuntimeError. Ensure .env is created and env vars are passed into containers via docker-compose.yml.
- File not found: /data/nyc-taxi-trip-durationYYYY-MM-DD.csv â†’ verify the dated file exists and that ./data is mounted to /data in both webserver and scheduler.
- Canâ€™t access UI on :8080 â†’ ensure the container is healthy and the port isnâ€™t in use.
- Postgres table not appearing â†’ confirm task_load_to_postgres ran and that the connection string is reachable from within containers.
